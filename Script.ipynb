{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_text"
      ],
      "metadata": {
        "id": "9CofvKu5cxxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_hub"
      ],
      "metadata": {
        "id": "rs6OkByLc1Q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "rdEaprorc54n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Wz3jUWutd5Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "import tensorflow_text  # noqa\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from keras.layers import TextVectorization\n",
        "from tensorflow.python.keras import metrics\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "\n",
        "\n",
        "class F1Score(metrics.Metric):\n",
        "    def _init_(self, name='f1_score', **kwargs):\n",
        "        super()._init_(name=name, **kwargs)\n",
        "        self.precision = metrics.Precision()\n",
        "        self.recall = metrics.Recall()\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
        "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.precision.reset_state()\n",
        "        self.recall.reset_state()\n",
        "\n",
        "    def result(self):\n",
        "        precision = self.precision.result()\n",
        "        recall = self.recall.result()\n",
        "        return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))\n",
        "\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the saved pytorch model\n",
        "ner_model = torch.load('/content/drive/MyDrive/src/Model_3.pth', map_location=device)\n",
        "\n",
        "# Load the saved tokenizer for NER\n",
        "tokenizer = torch.load('/content/drive/MyDrive/src/Our_Token.pth', map_location=device)\n",
        "# Load TF-IDF category model\n",
        "category_model_TFIDF = tf.keras.models.load_model('model_text_clf_tokenizer80.h5', custom_objects={'F1Score': F1Score})\n",
        "\n",
        "# Load BERT Model and Tokenizer\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-uncased')\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "# Load BERT category model\n",
        "bert_category_model = tf.keras.models.load_model('model_text_clf_bert_80_FINAL.h5', custom_objects={'F1Score': F1Score})\n",
        "\n",
        "# Load the vocabulary from a file\n",
        "with open('vocabulary.txt', 'r') as f:\n",
        "    vocabulary = [line.strip() for line in f]\n",
        "\n",
        "# Load the categories from a file\n",
        "with open('categories.txt', 'r') as f:\n",
        "    categories = [line.strip() for line in f]\n",
        "\n",
        "# Create a new TextVectorization layer and adapt it using the saved vocabulary\n",
        "text_vectorizer = TextVectorization(max_tokens=len(vocabulary), ngrams=2, output_mode=\"tf_idf\")\n",
        "text_vectorizer.adapt(vocabulary)\n",
        "tag_values = ['WP', 'VBG', 'RRB', 'IN', 'JJ', 'PDT', 'NNPS', 'VBZ', 'RB', 'VBD', 'EX', 'JJS', 'LRB', 'FW', 'CC', '.',\n",
        "              'JJR', 'NNP', 'VBN', 'CD', 'NNS', 'DT', 'VB', 'POS', 'WDT', 'MD', '$', 'RP', ',', 'PRP', 'VBP', 'NN', ':',\n",
        "              'PRP$', 'RBS', 'UH', 'WRB', 'WP$', '``', 'RBR', ';', 'TO', 'PAD']\n",
        "\n",
        "# For the unsupervised method\n",
        "\n",
        "# Loading models from tfhub.dev\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "# Constructing model to encode texts into high-dimensional vectors\n",
        "sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "encoder_inputs = preprocessor(sentences)\n",
        "sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "model = tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "\n",
        "\n",
        "index_category = {0:'Environment and climate resilience',1:'Mobility (transport)',2:'Local identity',3:'Future of work',4:'Land use'}\n",
        "\n",
        "\n",
        "def test_model(test_sentence):\n",
        "    tokenized_sentence = tokenizer.encode(test_sentence)\n",
        "    input_ids = torch.tensor([tokenized_sentence]).to(device)\n",
        "    with torch.no_grad():\n",
        "        output = ner_model(input_ids)\n",
        "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "    new_tokens, new_labels = [], []\n",
        "    for token, label_idx in zip(tokens, label_indices[0]):\n",
        "        if token.startswith(\"##\"):\n",
        "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "        else:\n",
        "            new_labels.append(tag_values[label_idx])\n",
        "            new_tokens.append(token)\n",
        "    ans = \"\"\n",
        "    for token, label in zip(new_tokens, new_labels):\n",
        "        ans += \"{}\\t{}\".format(label, token)\n",
        "        ans += \"\\n\"\n",
        "    nouns = re.findall(r'NN\\w*\\s+(\\w+)', ans)\n",
        "    return nouns\n",
        "\n",
        "\n",
        "def predict_category_TFIDF(sentence):\n",
        "    tokenized_sentence = text_vectorizer([sentence])\n",
        "    category_vector = category_model_TFIDF.predict(tokenized_sentence)\n",
        "    category_indices = np.where(category_vector[0] > 0.5)[0]\n",
        "    category_names = [categories[i] for i in category_indices]\n",
        "    return category_names\n",
        "\n",
        "\n",
        "def predict_category_bert(sentence):\n",
        "    inputs = bert_tokenizer.encode_plus(\n",
        "        sentence,\n",
        "        add_special_tokens=True,\n",
        "        max_length=20,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"tf\",\n",
        "        truncation=True\n",
        "    )\n",
        "    with tf.GradientTape() as tape:\n",
        "        inputs = {k: v for k, v in inputs.items()}\n",
        "        tape.watch(inputs['input_ids'])\n",
        "        outputs = bert_model(inputs)\n",
        "        sentence_representation = outputs[1]\n",
        "\n",
        "    category_vector = bert_category_model.predict(sentence_representation)\n",
        "    if np.all(category_vector < 0.5):\n",
        "        category_indices = [np.argmax(category_vector)]\n",
        "    else:\n",
        "        category_indices = np.where(category_vector[0] > 0.5)[0]\n",
        "    category_names = [categories[i] for i in category_indices]\n",
        "    return category_names\n",
        "\n",
        "def predict_category_all_sentence(sentence:str):\n",
        "\n",
        "    TRESHOLD = 0.25\n",
        "\n",
        "    # Encoding the messages and the categories sentences.\n",
        "    messages_sentences = tf.constant([sentence])\n",
        "    categories_sentences = tf.constant([\"Environment and climate resilience\", \"Mobility (transport)\", \"Local identity\", \"Future of work\", \"Land use\"])\n",
        "\n",
        "    messages_embeds = model(messages_sentences)\n",
        "    categories_embeds = model(categories_sentences)\n",
        "\n",
        "    # Messages-categories similarity\n",
        "    result = tf.tensordot(messages_embeds, categories_embeds, axes=[[1], [1]])\n",
        "    \n",
        "    category = ''\n",
        "    counter = 0\n",
        "    for value in result: # result = [[3432 34234 234 324234 23]]\n",
        "      for i,v in enumerate(value): # for each number in the list\n",
        "        if float(v) > TRESHOLD: # needs to be change accorindg to the result from ChatGPT\n",
        "          if counter > 0:\n",
        "            category += ', ' + index_category.get(i)\n",
        "          else: \n",
        "            category += index_category.get(i)\n",
        "            counter += 1\n",
        "    \n",
        "    if category == '':\n",
        "      return 'Other'\n",
        "    return category\n",
        "\n",
        "def predict_category_concatenated_nouns(nouns:list ,test_sentence:str):\n",
        "\n",
        "    TRESHOLD = 0.27\n",
        "\n",
        "    conc_str = ' '.join(nouns)\n",
        "\n",
        "    # Encoding the messages and the categories sentences.\n",
        "    messages_sentences = tf.constant([conc_str])\n",
        "    categories_sentences = tf.constant([\"Environment and climate resilience\", \"Mobility (transport)\", \"Local identity\", \"Future of work\", \"Land use\"])\n",
        "\n",
        "    messages_embeds = model(messages_sentences)\n",
        "    categories_embeds = model(categories_sentences)\n",
        "\n",
        "    # Messages-categories similarity\n",
        "    result = tf.tensordot(messages_embeds, categories_embeds, axes=[[1], [1]])\n",
        "\n",
        "    category = ''\n",
        "    counter = 0\n",
        "    for value in result: # result = [[3432 34234 234 324234 23]]\n",
        "      for i,v in enumerate(value): # for each number in the list\n",
        "        if float(v) > TRESHOLD:\n",
        "          if counter > 0:\n",
        "            category += ', ' + index_category.get(i)\n",
        "          else: \n",
        "            category += index_category.get(i)\n",
        "            counter += 1\n",
        "\n",
        "    if category == '':\n",
        "      return 'Other'\n",
        "    return category\n",
        "\n",
        "def predict_category_avg_category_nouns(nouns: list,test_sentence:str):\n",
        "  \n",
        "    TRESHOLD = 0.25\n",
        "\n",
        "    sum_result_column = [0 for i in range(5)]\n",
        "\n",
        "    for noun in nouns:\n",
        "\n",
        "      # Encoding the messages and the categories sentences.\n",
        "      messages_sentences = tf.constant([noun])\n",
        "      categories_sentences = tf.constant([\"Environment and climate resilience\", \"Mobility (transport)\", \"Local identity\", \"Future of work\", \"Land use\"])\n",
        "\n",
        "      messages_embeds = model(messages_sentences)\n",
        "      categories_embeds = model(categories_sentences)\n",
        "\n",
        "      # Messages-categories similarity\n",
        "      result = tf.tensordot(messages_embeds, categories_embeds, axes=[[1], [1]])\n",
        "\n",
        "      for value in result: # result = [[3432 34234 234 324234 23]]\n",
        "        for i,v in enumerate(value): # for each number in the list\n",
        "          sum_result_column[i] += float(v)\n",
        "\n",
        "      \n",
        "    sum_result_column = [num / 5 for num in sum_result_column] # calac avg\n",
        "\n",
        "    category = ''\n",
        "    counter = 0\n",
        "    for i,value in enumerate(sum_result_column):\n",
        "      if float(value) > TRESHOLD:\n",
        "        if counter > 0:\n",
        "          category += ', ' + index_category.get(i)\n",
        "        else: \n",
        "          category += index_category.get(i)\n",
        "          counter += 1\n",
        "      \n",
        "    if category == '':\n",
        "      return 'Other'\n",
        "    return category\n",
        "\n",
        "def process_file(input_file_path):\n",
        "    with open(input_file_path, 'r') as file:\n",
        "        test_sentence = file.read()\n",
        "    nouns = test_model(test_sentence)\n",
        "    \n",
        "    # supervised\n",
        "    categories_TFIDF = predict_category_TFIDF(test_sentence)\n",
        "    categories_BERT = predict_category_bert(test_sentence)\n",
        "\n",
        "    # unsupervied\n",
        "    categories_sent = predict_category_all_sentence(test_sentence)\n",
        "    categories_conc_nouns = predict_category_concatenated_nouns(nouns,test_sentence)\n",
        "    categories_nouns_avg = predict_category_avg_category_nouns(nouns,test_sentence)\n",
        "\n",
        "    with open(input_file_path, 'a') as file:\n",
        "        file.write(\"\\nThe sentence's nouns are: \" + ', '.join(nouns) + \"\\n\")\n",
        "        file.write(\"The categories of this sentence according to TF-IDF: \" + ', '.join(categories_TFIDF) + \"\\n\")\n",
        "        file.write(\"The categories of this sentence according to BERT: \" + ', '.join(categories_BERT) + \"\\n\")\n",
        "        file.write(\"The categories of this sentence according to unsuper-sentence: \" + categories_sent + \"\\n\")\n",
        "        file.write(\"The categories of this sentence according to unsuper-conc-nouns: \" + categories_conc_nouns + \"\\n\")\n",
        "        file.write(\"The categories of this sentence according to unsuper-nouns-avg: \" + categories_nouns_avg + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "# input_file_path = sys.argv[1]\n",
        "process_file('/content/sentence.txt')"
      ],
      "metadata": {
        "id": "rb1hGuqUl20z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}