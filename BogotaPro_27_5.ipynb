{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94XSUYD-xMN2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers tensorflow-text xlsxwriter pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "O6kWMjDhxUHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oVNZMYf0CSs"
      },
      "source": [
        "# Preparation\n",
        "- Create a function that create excel to hold the results of the models (Will be used later).\n",
        "- Extract the relevant data (the client sentences) from the dataset and hold it in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EGUwmtvbF77V"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def creating_excel() -> pd.DataFrame:\n",
        "    # Set the column names\n",
        "    data = {'Sentence': [],'category': []}\n",
        "\n",
        "    # Create an empty DataFrame.\n",
        "    output_excel = pd.DataFrame(data)\n",
        "    print('DataFrame Created')\n",
        "    return output_excel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-45XRDMAKmyr"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "from xlsxwriter import Workbook\n",
        "\n",
        "input_excel = pandas.read_csv('/content/drive/MyDrive/src/GPTClassification.csv') # data set\n",
        "\n",
        "client_message = [] # 364 sentence by client with nouns\n",
        "\n",
        "# rows num\n",
        "n_rows = len(input_excel.index)\n",
        "\n",
        "# columns num\n",
        "n_cols_ = len(input_excel.columns)\n",
        "\n",
        "\n",
        "for row in range(n_rows):\n",
        "    message = input_excel.iloc[row][0]\n",
        "    client_message.append(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGFkFeK79dyK"
      },
      "source": [
        "\n",
        "\n",
        "# Unsupervised  classification\n",
        "\n",
        "In unsupervised classification, the algorithm tries to identify patterns or structures within the data by analyzing the similarities and differences between data points. The goal is to partition the data into groups or clusters based on their similarity, so that data points within the same cluster are more similar to each other than to those in other clusters.\n",
        "\n",
        "- First step: We use Named Entity Recognition with Bert module to get the nouns in the sentence.\n",
        "- Second Step: We use smaller-LaBSE(Language-agnostic BERT Sentence Embedding) model to get the sentences embeddings.\n",
        "\n",
        "A note about smaller-LaBSE : LaBSE is a pre-trained model that produces fixed-length vectors to represent input sentences in a way that preserves their semantic meaning across languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4151_4JyDW3W"
      },
      "source": [
        "# First step\n",
        "We use Named Entity Recognition with Bert module to get the nouns in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sV-OsoJT9_D_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the model and the tokenizer from the downloaded files.\n",
        "# I added 'map_location=torch.device('cpu')' bcz I use only cpu\n",
        "model = torch.load(r\"/content/drive/MyDrive/src/my_model_3.pth\", map_location=torch.device('cpu'))\n",
        "tokenizer = torch.load(r\"/content/drive/MyDrive/src/my_tokenizer.pth\", map_location=torch.device('cpu'))\n",
        "\n",
        "all_sentence_nouns = []\n",
        "\n",
        "# Our input\n",
        "for sentence in client_message:\n",
        "  tokenized_sentence = tokenizer.encode(sentence)  # list of numbers represent each word\n",
        "  input_ids = torch.tensor([tokenized_sentence])  # I removed the .cuda() bcz I used only cpu on my computer\n",
        "\n",
        "\n",
        "  tag_values = ['DT', 'POS', 'NNS', 'VBG', 'CD', ';', 'JJS', 'NN', 'RP', '.', 'WP', 'PRP', 'CC', 'WRB', 'RBR', 'MD', 'VBZ', 'UH', 'FW', 'PDT',\n",
        "                'NNP', ':', 'JJ', 'JJR', 'RRB', '$', 'VB', ',', 'VBP', 'PRP$', 'NNPS', '``', 'IN', 'EX', 'TO', 'RB', 'VBN', 'RBS', 'WDT', 'LRB', 'VBD', 'WP$', 'PAD']\n",
        "\n",
        "  with torch.no_grad():\n",
        "      output = model(input_ids)\n",
        "\n",
        "  label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
        "\n",
        "  # join bpe split tokens\n",
        "  tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "\n",
        "  new_tokens, new_labels, nouns_from_sentence = [] ,[], []\n",
        "\n",
        "  for token, label_idx in zip(tokens, label_indices[0]):\n",
        "      if token.startswith(\"##\"):\n",
        "          new_tokens[-1] = new_tokens[-1] + token[2:]\n",
        "      else:\n",
        "          new_labels.append(tag_values[label_idx])\n",
        "          new_tokens.append(token)\n",
        "\n",
        "  for token, label in zip(new_tokens, new_labels):\n",
        "      if 'NN' in label and '[SEP]' not in token and '[CLS]' not in token and '?' not in token:\n",
        "          nouns_from_sentence.append(token)\n",
        "\n",
        "  all_sentence_nouns.append(nouns_from_sentence) # [['noun1','noun2',..],[]]\n",
        "\n",
        "print(f'all_sentence_nouns = {all_sentence_nouns}')\n",
        "print(f'client_message = {client_message}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "td_YbOotybIt"
      },
      "source": [
        "# Second step\n",
        "We use smaller-LaBSE(Language-agnostic BERT Sentence Embedding) model to get the sentences embeddings.\n",
        "\n",
        "We have 3 options:\n",
        "1. Get the vector for *all* the sentence.\n",
        "2. Get the vector for a *concatenation string of the nouns* in the sentence.\n",
        "3. Get a vector *for each noun in the sentence* and sum the values for each category you get and then calc the avg of all the nouns in the sentence and return the max.\n",
        "\n",
        "For each option we check what category the sentence belong to (by calc the arithmetic distance between the vector that represent the sentence and the vectors that represent the categories).\n",
        "\n",
        "# The first option\n",
        "Get the vector for *all* the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOyZfCFpDcC0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text \n",
        "import tensorflow_hub as hub\n",
        "from xlsxwriter import Workbook\n",
        "\n",
        "TRESHOLD = 0.25\n",
        "\n",
        "# Loading models from tfhub.dev\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "# Constructing model to encode texts into high-dimensional vectors\n",
        "sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "encoder_inputs = preprocessor(sentences)\n",
        "sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "model = tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "\n",
        "# Start Algo\n",
        "\n",
        "index_category = {0:'Environment and climate resilience',1:'Mobility (transport)',2:'Local identity',3:'Future of work',4:'Land use'}\n",
        "\n",
        "output_excel = creating_excel()  # create an Excel file\n",
        "excel_index = 0\n",
        "\n",
        "for sentence in client_message:\n",
        "  # Encoding the messages and the categories sentences.\n",
        "  messages_sentences = tf.constant([sentence])\n",
        "  categories_sentences = tf.constant([\"Environment and climate resilience\", \"Mobility (transport)\", \"Local identity\", \"Future of work\", \"Land use\"])\n",
        "\n",
        "  messages_embeds = model(messages_sentences)\n",
        "  categories_embeds = model(categories_sentences)\n",
        "\n",
        "  # Messages-categories similarity\n",
        "  result = tf.tensordot(messages_embeds, categories_embeds, axes=[[1], [1]])\n",
        "  \n",
        "\n",
        "  # write the sentence in the excel\n",
        "  output_excel.loc[excel_index, 'Sentence'] = sentence\n",
        "\n",
        "  category = ''\n",
        "  counter = 0\n",
        "  for value in result: # result = [[3432 34234 234 324234 23]]\n",
        "    for i,v in enumerate(value): # for each number in the list\n",
        "      if float(v) > TRESHOLD: # needs to be change accorindg to the result from ChatGPT\n",
        "        if counter > 0:\n",
        "          category += ', ' + index_category.get(i)\n",
        "        else: \n",
        "          category += index_category.get(i)\n",
        "          counter += 1\n",
        "  \n",
        "  output_excel.loc[excel_index, 'category'] = category\n",
        "  excel_index += 1\n",
        "\n",
        "output_excel.to_excel(\"Results1.xlsx\", index=False)  # save the Excel file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWFDEEvUoHaB"
      },
      "source": [
        "# The second option\n",
        "Get the vector for a *concatenation string of the nouns* in the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81Q-7zojoGJq"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text  # noqa\n",
        "import tensorflow_hub as hub\n",
        "from xlsxwriter import Workbook\n",
        "\n",
        "TRESHOLD = 0.27\n",
        "\n",
        "# Loading models from tfhub.dev\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "# Constructing model to encode texts into high-dimensional vectors\n",
        "sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "encoder_inputs = preprocessor(sentences)\n",
        "sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "model = tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "\n",
        "# Start Algo\n",
        "index_category = {0:'Environment and climate resilience',1:'Mobility (transport)',2:'Local identity',3:'Future of work',4:'Land use'}\n",
        "results_vectors = []\n",
        "\n",
        "output_excel = creating_excel()  # create an Excel file\n",
        "excel_index = 0\n",
        "\n",
        "\n",
        "for nouns,sentence in zip(all_sentence_nouns,client_message):\n",
        "\n",
        "  # write the sentence in the excel\n",
        "  output_excel.loc[excel_index, 'Sentence'] = sentence\n",
        "\n",
        "  # when list of nouns is empty, continue to the next iteration\n",
        "  if len(nouns) == 0:\n",
        "    output_excel.loc[excel_index, 'category'] = ''\n",
        "    excel_index += 1\n",
        "    continue\n",
        "\n",
        "  # creates a concatenated string of all nouns\n",
        "  conca_string = ' '.join(nouns)\n",
        "\n",
        "  # Encoding the messages and the categories sentences.\n",
        "  messages_sentences = tf.constant([conca_string])\n",
        "  categories_sentences = tf.constant([\"Environment and climate resilience\", \"Mobility (transport)\", \"Local identity\", \"Future of work\", \"Land use\"])\n",
        "\n",
        "  messages_embeds = model(messages_sentences)\n",
        "  categories_embeds = model(categories_sentences)\n",
        "\n",
        "  # Messages-categories similarity\n",
        "  result = tf.tensordot(messages_embeds, categories_embeds, axes=[[1], [1]])\n",
        "  results_vectors.append(result[0]) # save all the vectors for concatenated nouns\n",
        "\n",
        "  category = ''\n",
        "  counter = 0\n",
        "  for value in result: # result = [[3432 34234 234 324234 23]]\n",
        "    for i,v in enumerate(value): # for each number in the list\n",
        "      if float(v) > TRESHOLD: # needs to be change accorindg to the result from ChatGPT\n",
        "        if counter > 0:\n",
        "          category += ', ' + index_category.get(i)\n",
        "        else: \n",
        "          category += index_category.get(i)\n",
        "          counter += 1\n",
        "\n",
        "  output_excel.loc[excel_index, 'category'] = category\n",
        "  excel_index += 1\n",
        "\n",
        "output_excel.to_excel(\"Results2.xlsx\", index=False)  # save the Excel file\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuE9qsiE5VU-"
      },
      "source": [
        "# The third option\n",
        "Get a vector *for each noun in the sentence* and sum the values for each category you get and then calc the avg of all the nouns in the sentence and return the max:\n",
        "\n",
        "**example:**\n",
        "\n",
        "  sentence = \"I think there should be many various sitting and studying    places for students, both inside and outside of the building.\"\n",
        "\n",
        "  nouns = places students building\n",
        "\n",
        "  call the model on each noun -> we get [value1,value2,...,value5]\n",
        "\n",
        "  sum all the values for each categoty and then return the category with the max value.\n",
        "\n",
        "  \n",
        "\n",
        "smaller-LaBSE.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afTMBJ980aqG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text  # noqa\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "TRESHOLD = 0.25\n",
        "\n",
        "# Loading models from tfhub.dev\n",
        "encoder = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang/1\")\n",
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/jeongukjae/smaller_LaBSE_15lang_preprocess/1\")\n",
        "\n",
        "# Constructing model to encode texts into high-dimensional vectors\n",
        "sentences = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"sentences\")\n",
        "encoder_inputs = preprocessor(sentences)\n",
        "sentence_representation = encoder(encoder_inputs)[\"pooled_output\"]\n",
        "normalized_sentence_representation = tf.nn.l2_normalize(sentence_representation, axis=-1)  # for cosine similarity\n",
        "model = tf.keras.Model(sentences, normalized_sentence_representation)\n",
        "\n",
        "# Start Algo\n",
        "index_category = {0:'Environment and climate resilience',1:'Mobility (transport)',2:'Local identity',3:'Future of work',4:'Land use'}\n",
        "\n",
        "output_excel = creating_excel()  # create an Excel file\n",
        "excel_index = 0\n",
        "sum_result_column = [0 for i in range(5)]\n",
        "\n",
        "for nouns,sentence in zip(all_sentence_nouns,client_message):\n",
        "\n",
        "  output_excel.loc[excel_index, 'Sentence'] = sentence\n",
        "\n",
        "  # when list of nouns is empty\n",
        "  if len(nouns) == 0:\n",
        "    # write the category in the excel\n",
        "    output_excel.loc[excel_index, 'category'] = ''\n",
        "    excel_index += 1\n",
        "    continue\n",
        "\n",
        "  for noun in nouns:\n",
        "\n",
        "    # Encoding the messages and the categories sentences.\n",
        "    messages_sentences = tf.constant([noun])\n",
        "    categories_sentences = tf.constant([\"Environment and climate resilience\", \"Mobility (transport)\", \"Local identity\", \"Future of work\", \"Land use\"])\n",
        "\n",
        "    messages_embeds = model(messages_sentences)\n",
        "    categories_embeds = model(categories_sentences)\n",
        "\n",
        "    # Messages-categories similarity\n",
        "    result = tf.tensordot(messages_embeds, categories_embeds, axes=[[1], [1]])\n",
        "\n",
        "    for value in result: # result = [[3432 34234 234 324234 23]]\n",
        "      for i,v in enumerate(value): # for each number in the list\n",
        "        sum_result_column[i] += float(v)\n",
        "\n",
        "  print(f'before sum_result  {sum_result_column}')\n",
        "  sum_result_column = [num / 5 for num in sum_result_column] # calac avg\n",
        "  print(f'sum_result  {sum_result_column}')\n",
        "\n",
        "  category = ''\n",
        "  counter = 0\n",
        "  for i,value in enumerate(sum_result_column):\n",
        "    if float(value) > TRESHOLD:\n",
        "      if counter > 0:\n",
        "        category += ', ' + index_category.get(i)\n",
        "      else: \n",
        "        category += index_category.get(i)\n",
        "        counter += 1\n",
        "\n",
        "  output_excel.loc[excel_index, 'category'] = category\n",
        "  excel_index += 1\n",
        "  \n",
        "\n",
        "output_excel.to_excel(\"Results3.xlsx\", index=False)  # save the Excel file"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze the data Unsupervised\n",
        "* Convert the results from the unsupervised methods and ChatGpt to vectors such that each element of the vector represents a category (1 = in the category, 0 = no).\n",
        "* Check for each option the accuracy, ... (use the results of ChatGpt as a truth ground)"
      ],
      "metadata": {
        "id": "G4myhC1daytH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare ChatGPT results"
      ],
      "metadata": {
        "id": "Ti1nCi1sxWCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "def organize_data(path: str, is_csv: bool):\n",
        "  if is_csv:\n",
        "    df_original = pd.read_csv(path)\n",
        "  else:\n",
        "    df_original = pd.read_excel(path)\n",
        "\n",
        "  df_original = df_original[~df_original[\"Sentence\"].duplicated()]\n",
        "\n",
        "  # Replace \";\" with \",\"\n",
        "  df_original['category'] = df_original['category'].str.replace(';', ',')\n",
        "  df_original['category'] = df_original['category'].fillna('other') # replace null in category with \"other\"\n",
        "\n",
        "  # change the data in all the category column to be lists that containing the categories ['Land use', 'Mobil...]\n",
        "  df_original['category'] = df_original['category'].apply(lambda x: x.split(', '))\n",
        "  df_original['category'] = df_original['category'].apply(tuple)\n",
        "  return df_original\n"
      ],
      "metadata": {
        "id": "vYKgZBHHvk8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Organzie the DATA"
      ],
      "metadata": {
        "id": "wHMPUPtgsOjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# organize the data \n",
        "df_clean_GPT = organize_data('/content/drive/MyDrive/src/GPTClassification.csv', True)\n",
        "\n",
        "df_clean_option_1 = organize_data('/content/drive/MyDrive/src/Results1.xlsx', False)\n",
        "df_clean_option_2 = organize_data('/content/drive/MyDrive/src/Results2.xlsx',False)\n",
        "df_clean_option_3 = organize_data('/content/drive/MyDrive/src/Results3.xlsx',False)\n",
        "\n",
        "categories_GPT = tf.ragged.constant(df_clean_GPT[\"category\"].values)\n",
        "\n",
        "categories_option_1 = tf.ragged.constant(df_clean_option_1[\"category\"].values)\n",
        "categories_option_2 = tf.ragged.constant(df_clean_option_2[\"category\"].values)\n",
        "categories_option_3 = tf.ragged.constant(df_clean_option_3[\"category\"].values)\n",
        "\n",
        "# creating a vector to represent the labels\n",
        "lookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\", num_oov_indices=0)\n",
        "lookup.adapt(categories_GPT)\n",
        "vocab = lookup.get_vocabulary()\n",
        "\n",
        "def invert_multi_hot(encoded_labels):\n",
        "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
        "    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
        "    return np.take(vocab, hot_indices)\n",
        "\n",
        "\n",
        "label_binarized_chatGPT = lookup(categories_GPT).numpy()\n",
        "\n",
        "label_binarized_option_1 = lookup(categories_option_1).numpy()\n",
        "label_binarized_option_2 = lookup(categories_option_2).numpy()\n",
        "label_binarized_option_3 = lookup(categories_option_3).numpy()"
      ],
      "metadata": {
        "id": "xVcM_D2VWSG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1"
      ],
      "metadata": {
        "id": "iAgNBPZPczxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.keras import metrics\n",
        "from tensorflow.keras import metrics\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "\n",
        "print(f\"option 1:\\n\")\n",
        "\n",
        "for i in range(label_binarized_option_1.shape[1]):\n",
        "    print(f\"Category {i+1}:\\n\")\n",
        "    true_labels = label_binarized_chatGPT[:, i]\n",
        "    predicted_labels_1 = label_binarized_option_1[:, i]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels_1)\n",
        "    precision = precision_score(true_labels, predicted_labels_1)\n",
        "    recall = recall_score(true_labels, predicted_labels_1)\n",
        "    f1 = f1_score(true_labels, predicted_labels_1)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\\n\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predicted_labels_1)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title(f'Confusion matrix for category {i+1}')\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "W5K1fYLJZtL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2"
      ],
      "metadata": {
        "id": "cKmzQYyGdDXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(f\"option 2:\\n\")\n",
        "\n",
        "for i in range(label_binarized_option_2.shape[1]):\n",
        "    print(f\"Category {i+1}:\\n\")\n",
        "    true_labels = label_binarized_chatGPT[:, i]\n",
        "    predicted_labels_2 = label_binarized_option_2[:, i]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels_2)\n",
        "    precision = precision_score(true_labels, predicted_labels_2)\n",
        "    recall = recall_score(true_labels, predicted_labels_2)\n",
        "    f1 = f1_score(true_labels, predicted_labels_2)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\\n\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predicted_labels_2)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title(f'Confusion matrix for category {i+1}')\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Lvezv6Amc51a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 3"
      ],
      "metadata": {
        "id": "exk_W6H3dGLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "print(f\"option 3:\\n\")\n",
        "\n",
        "for i in range(label_binarized_option_3.shape[1]):\n",
        "    print(f\"Category {i+1}:\\n\")\n",
        "    true_labels = label_binarized_chatGPT[:, i]\n",
        "    predicted_labels_3 = label_binarized_option_3[:, i]\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels_3)\n",
        "    precision = precision_score(true_labels, predicted_labels_3)\n",
        "    recall = recall_score(true_labels, predicted_labels_3)\n",
        "    f1 = f1_score(true_labels, predicted_labels_3)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1 Score: {f1}\\n\")\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predicted_labels_3)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "    plt.title(f'Confusion matrix for category {i+1}')\n",
        "    plt.ylabel('Actual label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QeQLcbxndBgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}